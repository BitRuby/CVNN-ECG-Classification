{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "865721f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Dropout\n",
    "from tensorflow import convert_to_tensor, float32\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import mlflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from imblearn.under_sampling import RandomUnderSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bda17658",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Dataset/mitbih_database\"\n",
    "filenames = next(os.walk(path))[2]\n",
    "records=list()\n",
    "annotations=list()\n",
    "filenames.sort()\n",
    "for f in filenames:\n",
    "    filename, file_extension = os.path.splitext(f)\n",
    "    if(file_extension=='.csv'):\n",
    "        records.append(path+'/'+filename+file_extension)\n",
    "    else:\n",
    "        annotations.append(path+'/'+filename+file_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c4f976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_record_signals(index):\n",
    "    signals = []\n",
    "    labels = []\n",
    "    with open(records[index],'r') as csvfile:\n",
    "        filereader = csv.reader(csvfile,delimiter=',',quotechar='|')\n",
    "        row_index = -1\n",
    "        for row in filereader:\n",
    "            if(row_index >= 0):\n",
    "                signals.insert(row_index, int(row[1]))\n",
    "            row_index += 1\n",
    "        signals = np.array(signals)\n",
    "    with open(annotations[index],'r') as csvfile:\n",
    "        filereader = csv.reader(csvfile,delimiter=',',quotechar='|')\n",
    "        row_index = -1\n",
    "        for row in filereader:\n",
    "            if(row_index >= 0):\n",
    "                elements = list(filter(lambda x: len(x) > 0, row[0].split(\" \")))\n",
    "                labels.insert(row_index, [int(elements[1]), elements[2]])\n",
    "            row_index += 1\n",
    "        labels = np.array(labels)\n",
    "    return signals, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d2f9efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 650000\n",
      "Value Range: 611 -> 1538\n",
      "Classes: ['+' 'N' 'V' '~']\n"
     ]
    }
   ],
   "source": [
    "signals, labels = get_record_signals(6)\n",
    "print(\"Number of samples:\", len(signals))\n",
    "print(\"Value Range:\", np.min(signals), \"->\", np.max(signals))\n",
    "print(\"Classes:\", np.unique(labels[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "879b7b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(window_size):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    half_w = window_size // 2\n",
    "    valid_labels = {'A', 'L', 'N', 'R', 'V'}\n",
    "\n",
    "    for i in range(len(records)):\n",
    "        signals, labels = get_record_signals(i)\n",
    "        sig_len = len(signals)\n",
    "\n",
    "        mask = np.isin(labels[:, 1], list(valid_labels))\n",
    "        filtered_labels = labels[mask]\n",
    "\n",
    "        for label in filtered_labels:\n",
    "            center = int(label[0])\n",
    "            start = center - half_w\n",
    "            end = center + half_w\n",
    "\n",
    "            if start < 0 or end > sig_len:\n",
    "                continue\n",
    "\n",
    "            X.append(signals[start:end])\n",
    "            y.append(label[1])\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb2292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "WINDOW_SIZE = 250\n",
    "EPOCHS = 500\n",
    "BATCH_SIZE = 128\n",
    "TEST_SIZE = 0.25\n",
    "N_RUNS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c497c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# signals_list = [X[y.index(\"A\")], X[y.index(\"L\")], X[y.index(\"N\")], X[y.index(\"R\")], X[y.index(\"V\")]]\n",
    "# titles = [\"A - Atrial premature beat (APB)\", \"L - Left bundle branch block beat (LBBB)\", \"N - Normal beat\", \"R - Right bundle branch block beat (RBBB)\", \"V - Premature ventricular contraction (PVC)\"]\n",
    "\n",
    "# fig, ax = plt.subplots(1, 5, figsize=(20, 5))\n",
    "\n",
    "# for a, sig, title in zip(ax, signals_list, titles):\n",
    "#     a.plot(range(0, len(sig)), sig)\n",
    "#     a.set_title(title)\n",
    "#     a.set_xlabel(\"Sample\")\n",
    "#     a.grid(True)\n",
    "\n",
    "# ax[0].set_ylabel(\"Amplitude\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6c6ac54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\anaconda32\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 1s 2ms/step\n",
      "782/782 [==============================] - 2s 2ms/step\n",
      "782/782 [==============================] - 1s 2ms/step\n",
      "782/782 [==============================] - 1s 2ms/step\n",
      "782/782 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "for i in range(N_RUNS):\n",
    "    X, y = process_data(window_size=WINDOW_SIZE)\n",
    "    \n",
    "    # values, counts = np.unique(y, return_counts=True)\n",
    "    # plt.bar(values, counts)\n",
    "    # plt.title('Class Distribution')\n",
    "    # plt.xlabel('Class')\n",
    "    # plt.ylabel('Number of Samples')\n",
    "    # plt.show()\n",
    "    \n",
    "    mlflow.start_run()\n",
    "    mlflow.set_experiment(\"DNN_MIT_BIH_Arrythmia_Classification\")\n",
    "    mlflow.log_param(\"model\", \"DNN-Dense-32-Softmax\")\n",
    "    mlflow.log_param(\"input_dim\", WINDOW_SIZE)\n",
    "    mlflow.log_param(\"epochs\", EPOCHS)\n",
    "    mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
    "    mlflow.log_param(\"optimizer\", \"adam\")\n",
    "    mlflow.log_param(\"loss\", \"categorical_crossentropy\")\n",
    "    mlflow.log_param(\"test_size\", TEST_SIZE)\n",
    "    mlflow.log_param(\"scaler\", \"MinMaxScaler\")\n",
    "    mlflow.log_param(\"classes\", \"A,L,N,R,V\")\n",
    "    mlflow.log_param(\"window_size\", WINDOW_SIZE)\n",
    "    # Train/test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)\n",
    "    \n",
    "    #Label Binarization\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    y_train = lb.fit_transform(y_train)\n",
    "    y_test = lb.transform(y_test)\n",
    "    \n",
    "    # Under-sampling\n",
    "    rus = RandomUnderSampler(sampling_strategy='auto')\n",
    "    X_train_res, y_train_res = rus.fit_resample(X_train, y_train)\n",
    "    \n",
    "    #Min/Max Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_res = scaler.fit_transform(X_train_res)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    # print(f\"Using device: {device}\")\n",
    "    # print(type(X_test))\n",
    "    # print(X_test.dtype)\n",
    "    # print(X_test.shape)\n",
    "    # print(type(X_test[0]))\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(WINDOW_SIZE,), name=\"InputLayer\"))\n",
    "    model.add(Dense(units=32, activation=\"relu\", name=f\"HiddenLayer-1\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'],\n",
    "                    )\n",
    "    history = model.fit(X_train_res, y_train_res,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE, verbose=0)\n",
    "        \n",
    "    X_test_tf = convert_to_tensor(X_test, dtype=float32)\n",
    "    y_test_tf = convert_to_tensor(y_test)\n",
    "\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None\n",
    "    )\n",
    "\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"macro\"\n",
    "    )\n",
    "\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"weighted\"\n",
    "    )\n",
    "\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    mlflow.log_metric(\"precision_macro\", precision_macro)\n",
    "    mlflow.log_metric(\"recall_macro\", recall_macro)\n",
    "    mlflow.log_metric(\"f1_macro\", f1_macro)\n",
    "\n",
    "    mlflow.log_metric(\"precision_weighted\", precision_weighted)\n",
    "    mlflow.log_metric(\"recall_weighted\", recall_weighted)\n",
    "    mlflow.log_metric(\"f1_weighted\", f1_weighted)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_norm = cm / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    sns.heatmap(\n",
    "        cm_norm,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cmap=\"Blues\"\n",
    "    )\n",
    "    plt.title(\"Normalized Confusion Matrix\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"confusion_matrix.png\")\n",
    "    plt.close()\n",
    "\n",
    "    mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    with open(\"classification_report.txt\", \"w\") as f:\n",
    "        f.write(report)\n",
    "    mlflow.log_artifact(\"classification_report.txt\")\n",
    "    mlflow.end_run()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be3f5441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params.window_size params.epochs  accuracy_mean  accuracy_std  \\\n",
      "0                150           100       0.821878      0.053662   \n",
      "1                200           100       0.840625      0.096149   \n",
      "2                250           100       0.885184      0.024113   \n",
      "3                250          1000       0.921682      0.043035   \n",
      "4                250           250       0.876635      0.071668   \n",
      "5                250            30       0.779502      0.053969   \n",
      "6                250           500       0.941440      0.005831   \n",
      "7                300           100       0.832426      0.060128   \n",
      "\n",
      "   f1_macro_mean  f1_macro_std  \n",
      "0       0.738300      0.033209  \n",
      "1       0.763278      0.057163  \n",
      "2       0.789733      0.023002  \n",
      "3       0.841315      0.059127  \n",
      "4       0.802491      0.055297  \n",
      "5       0.711360      0.025527  \n",
      "6       0.866624      0.009167  \n",
      "7       0.731862      0.046838  \n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"DNN_MIT_BIH_Arrythmia_Classification\"\n",
    "\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_names=[experiment_name],\n",
    "    output_format=\"pandas\"\n",
    ")\n",
    "\n",
    "summary = (\n",
    "    runs\n",
    "    .groupby([\"params.window_size\", \"params.epochs\"])\n",
    "    .agg(\n",
    "        accuracy_mean=(\"metrics.accuracy\", \"mean\"),\n",
    "        accuracy_std=(\"metrics.accuracy\", \"std\"),\n",
    "        f1_macro_mean=(\"metrics.f1_macro\", \"mean\"),\n",
    "        f1_macro_std=(\"metrics.f1_macro\", \"std\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ada7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
