{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "865721f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Dropout\n",
    "from tensorflow import convert_to_tensor, float32\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import mlflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from scipy import signal\n",
    "import pywt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bda17658",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Dataset/mitbih_database\"\n",
    "filenames = next(os.walk(path))[2]\n",
    "records=list()\n",
    "annotations=list()\n",
    "filenames.sort()\n",
    "for f in filenames:\n",
    "    filename, file_extension = os.path.splitext(f)\n",
    "    if(file_extension=='.csv'):\n",
    "        records.append(path+'/'+filename+file_extension)\n",
    "    else:\n",
    "        annotations.append(path+'/'+filename+file_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0c4f976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_record_signals(index):\n",
    "    signals = []\n",
    "    labels = []\n",
    "    with open(records[index],'r') as csvfile:\n",
    "        filereader = csv.reader(csvfile,delimiter=',',quotechar='|')\n",
    "        row_index = -1\n",
    "        for row in filereader:\n",
    "            if(row_index >= 0):\n",
    "                signals.insert(row_index, int(row[1]))\n",
    "            row_index += 1\n",
    "        signals = np.array(signals)\n",
    "    with open(annotations[index],'r') as csvfile:\n",
    "        filereader = csv.reader(csvfile,delimiter=',',quotechar='|')\n",
    "        row_index = -1\n",
    "        for row in filereader:\n",
    "            if(row_index >= 0):\n",
    "                elements = list(filter(lambda x: len(x) > 0, row[0].split(\" \")))\n",
    "                labels.insert(row_index, [int(elements[1]), elements[2]])\n",
    "            row_index += 1\n",
    "        labels = np.array(labels)\n",
    "    return signals, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d684af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_detrend_and_butterworth(signals):\n",
    "    fs = 360.0\n",
    "    N = 650000\n",
    "    T = N / fs\n",
    "    t = np.linspace(0, T, N, endpoint=False)\n",
    "\n",
    "    # Detrend (usuniecie DC/linearna skÅ‚adowa)\n",
    "    data_detrended = signal.detrend(signals)\n",
    "\n",
    "    # Butterworth bandpass (np. 0.5 - 40 Hz)\n",
    "    lowcut = 0.5\n",
    "    highcut = 40.0\n",
    "    nyq = 0.5 * fs\n",
    "    b, a = signal.butter(4, [lowcut/nyq, highcut/nyq], btype='band')\n",
    "    data_filt = signal.filtfilt(b, a, data_detrended)  # zero-phase\n",
    "    return data_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "060da5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wavelet_features(data):\n",
    "    coeffs = pywt.wavedec(data, 'sym4', level=4)\n",
    "\n",
    "    features = []\n",
    "    for c in coeffs[1:]:\n",
    "        features.append(np.sum(c**2))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a7232360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_wavelet(data): \n",
    "    w = pywt.Wavelet('sym4')\n",
    "    maxlev = pywt.dwt_max_level(len(data), w.dec_len)\n",
    "    threshold = 0.04 # Threshold for filtering\n",
    "\n",
    "    coeffs = pywt.wavedec(data, 'sym4', level=maxlev)\n",
    "    for i in range(1, len(coeffs)):\n",
    "        coeffs[i] = pywt.threshold(coeffs[i], threshold*max(coeffs[i]))\n",
    "        \n",
    "    datarec = pywt.waverec(coeffs, 'sym4')\n",
    "    return datarec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "66fb5c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_welch(data):\n",
    "    fs = 360.0\n",
    "    nperseg = 64\n",
    "    noverlap = nperseg // 2\n",
    "    f_welch, Pxx = signal.welch(data, fs=fs, nperseg=nperseg, noverlap=noverlap, window='hann')\n",
    "    return f_welch, Pxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c3e0b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fft(data):\n",
    "    fs = 360.0\n",
    "    N = len(data)\n",
    "    X = np.fft.fft(data)\n",
    "    freqs = np.fft.fftfreq(N, 1/fs)\n",
    "    positive = freqs >= 0\n",
    "    freqs_pos = freqs[positive]\n",
    "    X_pos = np.abs(X[positive]) / N\n",
    "    return X_pos, freqs_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3d2f9efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 650000\n",
      "Value Range: 611 -> 1538\n",
      "Classes: ['+' 'N' 'V' '~']\n"
     ]
    }
   ],
   "source": [
    "signals, labels = get_record_signals(6)\n",
    "print(\"Number of samples:\", len(signals))\n",
    "print(\"Value Range:\", np.min(signals), \"->\", np.max(signals))\n",
    "print(\"Classes:\", np.unique(labels[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "879b7b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(window_size):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    half_w = window_size // 2\n",
    "    valid_labels = {'A', 'L', 'N', 'R', 'V'}\n",
    "\n",
    "    for i in range(len(records)):\n",
    "        signals, labels = get_record_signals(i)\n",
    "        signals = apply_detrend_and_butterworth(signals)\n",
    "        sig_len = len(signals)\n",
    "\n",
    "        mask = np.isin(labels[:, 1], list(valid_labels))\n",
    "        filtered_labels = labels[mask]\n",
    "\n",
    "        for label in filtered_labels:\n",
    "            center = int(label[0])\n",
    "            start = center - half_w\n",
    "            end = center + half_w\n",
    "\n",
    "            if start < 0 or end > sig_len:\n",
    "                continue\n",
    "\n",
    "            wavelet_energy = apply_wavelet(signals[start:end])\n",
    "            fusion = [*signals[start:end], *wavelet_energy]\n",
    "            X.append(fusion)\n",
    "            y.append(label[1])\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ecb2292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "WINDOW_SIZE = 250\n",
    "EPOCHS = 500\n",
    "BATCH_SIZE = 128\n",
    "TEST_SIZE = 0.25\n",
    "N_RUNS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c497c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# signals_list = [X[y.index(\"A\")], X[y.index(\"L\")], X[y.index(\"N\")], X[y.index(\"R\")], X[y.index(\"V\")]]\n",
    "# titles = [\"A - Atrial premature beat (APB)\", \"L - Left bundle branch block beat (LBBB)\", \"N - Normal beat\", \"R - Right bundle branch block beat (RBBB)\", \"V - Premature ventricular contraction (PVC)\"]\n",
    "\n",
    "# fig, ax = plt.subplots(1, 5, figsize=(20, 5))\n",
    "\n",
    "# for a, sig, title in zip(ax, signals_list, titles):\n",
    "#     a.plot(range(0, len(sig)), sig)\n",
    "#     a.set_title(title)\n",
    "#     a.set_xlabel(\"Sample\")\n",
    "#     a.grid(True)\n",
    "\n",
    "# ax[0].set_ylabel(\"Amplitude\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f0bf8517",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run() # Ensure any previous runs are closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b6c6ac54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 2ms/step\n",
      "782/782 [==============================] - 2s 2ms/step\n",
      "782/782 [==============================] - 1s 2ms/step\n",
      "782/782 [==============================] - 2s 2ms/step\n",
      "782/782 [==============================] - 2s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "for i in range(N_RUNS):\n",
    "    X, y = process_data(window_size=WINDOW_SIZE)\n",
    "    \n",
    "    # values, counts = np.unique(y, return_counts=True)\n",
    "    # plt.bar(values, counts)\n",
    "    # plt.title('Class Distribution')\n",
    "    # plt.xlabel('Class')\n",
    "    # plt.ylabel('Number of Samples')\n",
    "    # plt.show()\n",
    "    \n",
    "    mlflow.start_run()\n",
    "    mlflow.set_experiment(\"DNN_MIT_BIH_Arrythmia_Classification_With_Detrend_and_Butterworth_and_Wavelet_Fusion\")\n",
    "    mlflow.log_param(\"model\", \"DNN-Dense-128-Dense-128-Softmax\")\n",
    "    mlflow.log_param(\"input_dim\", 500)\n",
    "    mlflow.log_param(\"epochs\", EPOCHS)\n",
    "    mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
    "    mlflow.log_param(\"optimizer\", \"adam\")\n",
    "    mlflow.log_param(\"loss\", \"categorical_crossentropy\")\n",
    "    mlflow.log_param(\"test_size\", TEST_SIZE)\n",
    "    mlflow.log_param(\"scaler\", \"MinMaxScaler\")\n",
    "    mlflow.log_param(\"classes\", \"A,L,N,R,V\")\n",
    "    mlflow.log_param(\"window_size\", WINDOW_SIZE)\n",
    "    # Train/test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)\n",
    "    \n",
    "    #Label Binarization\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    y_train = lb.fit_transform(y_train)\n",
    "    y_test = lb.transform(y_test)\n",
    "    \n",
    "    # Under-sampling\n",
    "    rus = RandomUnderSampler(sampling_strategy='auto')\n",
    "    X_train_res, y_train_res = rus.fit_resample(X_train, y_train)\n",
    "    \n",
    "    #Min/Max Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_res = scaler.fit_transform(X_train_res)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    # print(f\"Using device: {device}\")\n",
    "    # print(type(X_test))\n",
    "    # print(X_test.dtype)\n",
    "    # print(X_test.shape)\n",
    "    # print(type(X_test[0]))\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(500,), name=\"InputLayer\"))\n",
    "    model.add(Dense(units=128, activation=\"relu\", name=f\"HiddenLayer-1\"))\n",
    "    model.add(Dense(units=128, activation=\"relu\", name=f\"HiddenLayer-2\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'],\n",
    "                    )\n",
    "    history = model.fit(X_train_res, y_train_res,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE, verbose=0)\n",
    "        \n",
    "    X_test_tf = convert_to_tensor(X_test, dtype=float32)\n",
    "    y_test_tf = convert_to_tensor(y_test)\n",
    "\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None\n",
    "    )\n",
    "\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"macro\"\n",
    "    )\n",
    "\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"weighted\"\n",
    "    )\n",
    "\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    mlflow.log_metric(\"precision_macro\", precision_macro)\n",
    "    mlflow.log_metric(\"recall_macro\", recall_macro)\n",
    "    mlflow.log_metric(\"f1_macro\", f1_macro)\n",
    "\n",
    "    mlflow.log_metric(\"precision_weighted\", precision_weighted)\n",
    "    mlflow.log_metric(\"recall_weighted\", recall_weighted)\n",
    "    mlflow.log_metric(\"f1_weighted\", f1_weighted)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_norm = cm / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    sns.heatmap(\n",
    "        cm_norm,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cmap=\"Blues\"\n",
    "    )\n",
    "    plt.title(\"Normalized Confusion Matrix\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"confusion_matrix.png\")\n",
    "    plt.close()\n",
    "\n",
    "    mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    with open(\"classification_report.txt\", \"w\") as f:\n",
    "        f.write(report)\n",
    "    mlflow.log_artifact(\"classification_report.txt\")\n",
    "    mlflow.end_run()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "be3f5441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  params.window_size params.epochs                     params.model  \\\n",
      "0                250           500  DNN-Dense-128-Dense-128-Softmax   \n",
      "\n",
      "   accuracy_mean  accuracy_std  f1_macro_mean  f1_macro_std  \n",
      "0       0.969577      0.007221       0.922215      0.017122  \n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"DNN_MIT_BIH_Arrythmia_Classification_With_Detrend_and_Butterworth_and_Wavelet_Fusion\"\n",
    "\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_names=[experiment_name],\n",
    "    output_format=\"pandas\"\n",
    ")\n",
    "\n",
    "summary = (\n",
    "    runs\n",
    "    .groupby([\"params.window_size\", \"params.epochs\", \"params.model\"])\n",
    "    .agg(\n",
    "        accuracy_mean=(\"metrics.accuracy\", \"mean\"),\n",
    "        accuracy_std=(\"metrics.accuracy\", \"std\"),\n",
    "        f1_macro_mean=(\"metrics.f1_macro\", \"mean\"),\n",
    "        f1_macro_std=(\"metrics.f1_macro\", \"std\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
